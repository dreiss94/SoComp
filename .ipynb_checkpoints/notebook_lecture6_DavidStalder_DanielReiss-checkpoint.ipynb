{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online data collection using the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please inlcude your names below\n",
    "# Also, please edit the name of the file and include the names of the two(or three) people answering\n",
    "\n",
    "# Pair answering the assignment: Daniel Reiss, David Stalder\n",
    "# Pair giving feedback: Christian Aeberhard, Ivan Allinckx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As step 0, pick your favorite Wikipedia page, open it in the browser, and then save it as an html file. Now open it in the browser as well as in a text editor and look at the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the requests library you can retrieve the html source of the page, in a response object (using requests.get(“url”)). The response object you received has content that you can access calling the .text function on it.\n",
    "\n",
    "Call text and save the result in a file, then open the file in a browser and check whether you successfully saved the page. Note, you will only be able to open the file in the browser if you give it an html extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Basic web crawling\n",
    "\n",
    "URLs have specific formats, for example any Wikipedia page will be of the format https://en.wikipedia.org/wiki/Pythonidae where the last word is the topic of the article.\n",
    "Next, we want to automate this saving process using the requests library and making automated requests to Wikipedia.\n",
    "\n",
    "Exercise: Pick 5 different words, and write code that loops through these words, and retrieves the html content for each associated wikipedia page, and saves the html text as wiki_htmls/[word].html files. (Choose words that actually have associated wiki pages). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "import requests\n",
    "res = requests.get(\"https://de.wikipedia.org/wiki/Iron_Maiden\")\n",
    "res.text\n",
    "names = ['Iron_Maiden', 'Dio_(band)','Rainbow_(rock_band)','Black_Sabbath','Ozzy_Osbourne']\n",
    "\n",
    "for i in names:\n",
    "    html = requests.get('https://en.wikipedia.org/wiki/'+i)\n",
    "    with open('wiki_htmls_'+i+'.html','w', encoding=\"utf-8\") as f:\n",
    "        f.write(html.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) URL formats\n",
    "\n",
    "What is the common URL in the case of Google searches? And in the case of Yelp? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google: https://www.google.com/search?q={Search Term}&oq={Search Term}&aqs={Browser}..69i57j0l7.1497j0j8&sourceid={Browser}&ie=UTF-8\n",
    "# Yelp: https://de.yelp.ch/z{location}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what happens to the URL if you want to define the location as well as the type of venue you are looking for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://de.yelp.ch/search?find_desc={venue}&find_loc={location}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find more search parameters for either of the two pages that you can define via the URLs? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps: https://www.google.com/maps/search/{Search Term}/@{Coordinates},14z/data=!3m1!4b1?hl={language}\n",
    "# Yelp for Company Owners: https://de.biz.yelp.ch/?source=consumer_site_header&utm_content=header&utm_medium=www&utm_source=cons_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) And now let's work with the HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BeautifulSoup parser library we will parse the webpage that you just saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import BeautifulSoup, our parser library\n",
    "# And make a soup object out of the html of the page\n",
    "\n",
    "# in case bs4 throws error try\n",
    "# !pip install --upgrade html5lib==1.0b8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    First paragraph.\n",
      "   </p>\n",
      "   <p class=\"inner-text\">\n",
      "    Second paragraph.\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    First outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    Second outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# print a nice version using prettify\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can find all instances of a tag at once: Try to predict what the following command will return: `soup.find_all('p')` and then call it to check if you were right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 First paragraph.\n",
       "             </p>, <p class=\"inner-text\">\n",
       "                 Second paragraph.\n",
       "             </p>, <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 Second outer paragraph.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the second element of this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                First outer paragraph.\n",
       "            </b>\n",
       "</p>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id=\"second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the text inside the second element of the list, using the .text on the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(id=\"second\").get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you try to find a specific element on a page you can reach it by finding classes or IDs of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 First outer paragraph.\n",
       "             </b>\n",
       " </p>, <p class=\"outer-text\">\n",
       " <b>\n",
       "                 Second outer paragraph.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How many elements would it return for 'inner_class'? Guess, and check your guess by using the find_all command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 First paragraph.\n",
       "             </p>, <p class=\"inner-text\">\n",
       "                 Second paragraph.\n",
       "             </p>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "soup.find_all('p', class_='inner-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4) Finding elements in the browser\n",
    "Since every web page is different and html can get very large and messy, the easiest way to find elements that you are interested in is to start from the browser window. So next we will quickly look at how to find elements using the developer tools in your browser. Open the following webpage in your browser (preferably Chrome): http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579#.Wkwh8VQ-fVo \n",
    "\n",
    "Find the developer tools in your browser. (In Chrome, it's view --> developer --> developer tools or Control+Shift+C on Windows and Command+Shift+C on Mac) You should end up with a panel at the bottom or the right side of the browser like what you see below. Make sure the Elements panel is highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to find a specific element, you can right click on it on the page and select \"inspect\". This will also open up the developer tools window. For example if we want to extract the current temperature value:\n",
    "\n",
    "<img src=\"inspect.png\">\n",
    "\n",
    "<img src=\"inspect_class.png\">\n",
    "\n",
    "<br><br>\n",
    "1. Using the find function, extract and print out the current temperature from the page. \n",
    "2. Do the same with the value in Celsius. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Fahrenheit is: 74°F.\n",
      "The current temperature in Celsius is: 23°C.\n"
     ]
    }
   ],
   "source": [
    "### Fill out and print a full sentence describing the temperature in F and C. \n",
    "temp_F = soup.find(class_=\"myforecast-current-lrg\").get_text()\n",
    "print('The current temperature in Fahrenheit is: ' + temp_F + '.')\n",
    "temp_C = soup.find(class_=\"myforecast-current-sm\").get_text()\n",
    "print('The current temperature in Celsius is: ' + temp_C + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this exercise we will extract each day's forecast from the 7 day extended forecast on the weather report page. <br>\n",
    "    a. Find the container for the seven day forecast on the weather page we just downloaded. <br>\n",
    "    b. Make a list with all forecast items (overnight, Wednesday, Wednesday night, etc) <br>\n",
    "    c. For each time period, print out the name of the period, the short description of the expected weather conditions, and the temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today   the weather will be Scattered Showers and Breezy and the temperature will be 81 °F.\n",
      "\n",
      "\n",
      "Tonight   the weather will be Isolated Showers and Breezy and the temperature will be 69 °F.\n",
      "\n",
      "\n",
      "Tuesday   the weather will be Isolated Showers and Breezy then Mostly Sunny and Windy and the temperature will be 80 °F.\n",
      "\n",
      "\n",
      "Tuesday Night the weather will be Isolated Showers and Windy and the temperature will be 70 °F.\n",
      "\n",
      "\n",
      "Wednesday   the weather will be Windy. Isolated Showers then Mostly Sunny and the temperature will be 81 °F.\n",
      "\n",
      "\n",
      "Wednesday Night the weather will be Scattered Showers and Windy and the temperature will be 69 °F.\n",
      "\n",
      "\n",
      "Thursday   the weather will be Scattered Showers and Windy and the temperature will be 81 °F.\n",
      "\n",
      "\n",
      "Thursday Night the weather will be Scattered Showers and Windy and the temperature will be 69 °F.\n",
      "\n",
      "\n",
      "Friday   the weather will be Scattered Showers and Breezy and the temperature will be 81 °F.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each time period print out something like: \n",
    "# Overnight the weather will be mostly clear and breezy and the temperature will be 65F.\n",
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "str_page = res.content.decode()\n",
    "str_split = '\\n<'.join(str_page.split('<'))\n",
    "str_split = '>\\n'.join(str_split.split('>'))\n",
    "str_split = str_split.replace('\\n', '')\n",
    "str_split = str_split.replace('<br>', ' ')\n",
    "soup = BeautifulSoup(str_split.encode(), 'html.parser')\n",
    "\n",
    "seven_day_forecast = soup.find(id=\"seven-day-forecast-list\")\n",
    "\n",
    "for i in seven_day_forecast:\n",
    "    if i.find(class_='temp temp-high'):\n",
    "        temp = i.find(class_='temp temp-high').get_text().replace('High: ', '')\n",
    "    else:\n",
    "        temp = i.find(class_='temp temp-low').get_text().replace('Low: ', '')\n",
    "    print(i.find(class_='period-name').get_text() + ' the weather will be ' + \n",
    "          i.find(class_='short-desc').get_text().replace('<\\br>', ' ') + ' and the temperature will be ' + \n",
    "          temp + '.\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Take a list of jobs (e.g.['teacher', 'lawyer', 'data-scientist']). For each job save the html of the result of searching it on indeed. The url of a result page looks like: https://www.indeed.com/q-data-scientist-jobs.html. \n",
    "<br>\n",
    "For each job find the names of the companies from the first result page.  Make a dictionary where the keys are the jobs and value is a list of the company names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'teacher': ['EduWorld China', 'Hamilton County Department Of Education', 'bValue', 'Edgenuity', \"The Coeur d'Alene Tribe\", 'Winchester Public Schools', 'SkySlate', 'Mosher Tech', 'Seneca Valley School District', 'Hamilton County Department Of Education'], 'lawyer': ['TransPerfect', 'Osano', 'Brenda A. Ray Law Offices', 'Axiom Law', 'Level 2 Legal Solutions', 'Michaels', 'US Department of State', 'Axiom Law', 'State of Minnesota Board of Public Defense', 'Kentucky Housing Corporation'], 'data-scientist': ['Pathrise', 'Hallmark', 'HBSE', 'Degreed', 'Bayer', 'Perrigo Company', 'Microsoft', 'Cisco Systems', 'HBO Max', 'kraken']}\n"
     ]
    }
   ],
   "source": [
    "job_list = ['teacher', 'lawyer', 'data-scientist']\n",
    "comp = []\n",
    "totalDict = {}\n",
    "for c in job_list:\n",
    "    res = requests.get(\"https://www.indeed.com/jobs?q=\" + c + \"&l=\")\n",
    "    soup = BeautifulSoup(res.content, 'html.parser').find_all(class_=\"company\")\n",
    "    comp.append(soup)\n",
    "    job_list = []\n",
    "    for j in soup:\n",
    "        job_list.append(j.get_text().replace('\\n', ''))\n",
    "    totalDict[c] = job_list\n",
    "    \n",
    "print(totalDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, we will do a 2 step crawling exercise. First, request the page for one chosen job category. Then make a list of the links to all specific job ads on that page. In a second step, crawl and save the content to all of these links. Name the folders and files in a meaningful way that helps you identify them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "jobCategory = 'teacher'\n",
    "linksList = []\n",
    "res = requests.get('https://www.indeed.com/jobs?q=' + jobCategory + '&l=')\n",
    "soup = BeautifulSoup(res.content, 'html.parser').find_all(class_=\"jobtitle turnstileLink\")\n",
    "os.makedirs('./' + jobCategory + '_jobs_found', exist_ok=True)\n",
    "for tag in soup:\n",
    "    linksList.append(tag['href'])\n",
    "for link in linksList:\n",
    "    res = requests.get(\"https://www.indeed.com\" + link)\n",
    "    htmlFile = io.open(jobCategory + \"_jobs_found/\"+ BeautifulSoup(res.content, 'html.parser').find(class_=\"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title\").get_text().replace(\"/\", \"\") + \".html\", \"w\", encoding=\"utf-8\")\n",
    "    htmlFile.write(res.text)\n",
    "    htmlFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every request you send has a so called HTTP header (unrelated to the content of the message), for example to communicate the size of the message, the browser from which the request is coming from, or what kind of response it is expecting back in the response. \n",
    "\n",
    "1) Read up on this: What parts does a request contain exactly and what is the purpose of a header? \n",
    "\n",
    "2) Look in the browser: Take a URL and find the request header using the developer tools in your browser. (Hint: you will need to look inside 'network'). \n",
    "\n",
    "3) If you don’t tell python otherwise, it will use a default header when sending requests. What is this default when you use the requests library?\n",
    "\n",
    "4) The requests library allows to specify the headers of your request exactly. Set the header of your request (for the  URL you previously picked) to be the one copied from your browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your chosen URL: ##\n",
    "\n",
    "Default header of Python requests: ##\n",
    "\n",
    "Header in your browser: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now compare the response headers for the same URL in the browser, and by calling a function on the response object in your code. What differences do you see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response header in your browser: ##\n",
    "\n",
    "Response header in the response in python: ##\n",
    "\n",
    "Difference: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for completing the first notebook! Now it’s time for feedback.\n",
    "1.\tPass your solution to the other pair in your group.\n",
    "2.\tInclude your feedback in the other pair’s notebook. Don’t forget to add your names at the top.\n",
    "3.\tReturn the notebook with feedback to the original pairs.\n",
    "4.\tUpload your notebook, with the feedback included by the other pair on OLAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of/suggest (among other things)\n",
    " - improvements in the code (e.g. readability, efficiency)\n",
    " - improvements in the answers (e.g. are they easy to understand, are they correct, how can they be improved?)\n",
    " - point out differences (e.g. are there any differences between the responses of the two pairs? if yes what are they, what is the cause, and in which way can they be useful?)\n",
    " \n",
    "In this specific notebook the questions to focus on for feedback are: 1, 2, 4 and 5. 3 was just an intro to parsing so no need to analyze in detail. Not all suggestions about the type of feedback apply to all types of questions, try to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below there is space for giving feedback. This space should be used only by the other pair in your group.\n",
    "\n",
    "'''\n",
    "Feedback here\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
